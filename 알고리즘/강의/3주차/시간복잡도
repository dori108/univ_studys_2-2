가상 컴터에서 가상언어로 작성된 가상 코드를 실행할 때 특정 입력에 대해 수행되는 알고리즘의 수행시간은 기본연산의 횟수로 수행시간을 정의한다.
문제는 입력의 종류가 무한하므로 모든 입력에 대해 수행시간을 측정하여 평균을 구하는 것은 현실적으로 가능하지 않다는 점이다. 따라서 최악의 경우의 입력을 가정하여 최악의 경우의 입력에 대한 알고리즘의 수행시간을 측정한다.

알고리즘의 기본연산의 횟수를 센다.
방금의 n개의 정수 중 최대값을 찾는 알고리즘에서 if문의 결과에 따라 current를 바꿔주는 수행은 될 수도 안될 수도 있음
최악의 경우는 이 연산이 n번 도는 경우로, 오름차순 정렬된 배열이 들어온다면 가능하겠다.

cp 기본연산 횟수

algorithm arraySum(A,B,n)
  sum =0
  for i =0 to n-1 do
    for j = n-1 down to i do
      sum+=A[i]*B[j]
    return sum
end_algorithm

위 알고리즘의 최악의 입력에 대해 수행하는 기본연산의 횟수는 ?

간단 표기법
빅오

최악의 입력에 대한 기본연산의 횟수를 정확히 세는 것은 귀찮아
대략적인 값만 알아도 좋아
가장 큰 항을 표시해줘 

*logn 시간 알고리즘
def number_of_bits(n):
    count = 0
    while n > 0:
        n = n // 2
        count += 1
    return count


*nlogn 시간 알고리즘
힙 머지 팀 

*n 시간 알고리즘
insertion

*n^2 시간 알고리즘
algorithm array_sum(A, B, n)
  sum = 0
  for i = 0 to n - 1 do
	for j = 0 to n - 1 do
	  sum += A[i]*B[j]
  return sum
end_algorithm

이중 for문이 들어가는 순간 n^2이 넘어간다
삼중은 당연히 n^3이겠지 

재귀함수는 2^n 알고리즘
-> 매~~~우 불적절한 알고리즘 효율이 좋지 않음

